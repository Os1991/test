{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f1a34594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import json\n",
    "import string\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "from razdel import tokenize\n",
    "\n",
    "import pymorphy2\n",
    "pd.options.mode.chained_assignment = None\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, classification_report, precision_recall_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6072aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = r'C:\\Users\\okruz\\Downloads\\nlp_test_task_2022\\dataset\\train.tsv'\n",
    "TEST_DATASET_PATH = r'C:\\Users\\okruz\\Downloads\\nlp_test_task_2022\\dataset\\test.tsv'\n",
    "df = pd.read_csv(DATASET_PATH, sep='\\t')\n",
    "test_df = pd.read_csv(TEST_DATASET_PATH, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8731b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True, keep='last')\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7f082883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#токенизируем текст и убираем окончания слов\n",
    "def text_modify(text):\n",
    "    \n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens]\n",
    "    \n",
    "    text=[stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "121d08db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].apply(lambda x: text_modify(x), 1)\n",
    "test_df['title'] = test_df['title'].apply(lambda x: text_modify(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d66578f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''отдельно считаем сколько раз слово встречалось в фейковых новостях и в настоящих. Затем вводим параметр фейковости слова.\n",
    "Чем чаще слово встречалось в фейковых новостях, тем значение больше. Если слово больше встречалось в реальных новостях, то\n",
    "значение отрицательное и, соответственно, также зависит от того, насколько чаще оно встречалось в реальных новостях\n",
    "'''\n",
    "\n",
    "def fake_rate(df):\n",
    "    df_fake = df.query('is_fake == 1').reset_index(drop=True)\n",
    "    df_real = df.query('is_fake == 0').reset_index(drop=True)\n",
    "    \n",
    "    for i in range(len(df_real['title'])):\n",
    "        df_real['title'][i] = ' '.join(df_real['title'][i])\n",
    "    \n",
    "    for i in range(len(df_fake['title'])):\n",
    "        df_fake['title'][i] = ' '.join(df_fake['title'][i])\n",
    "        \n",
    "    text_fake = np.array(df_fake.title.values)\n",
    "    text_real = np.array(df_real.title.values)\n",
    "    \n",
    "    text_fake = \" \".join(text_fake)\n",
    "    text_real = \" \".join(text_real)\n",
    "    \n",
    "    text_fake = text_fake.split()\n",
    "    text_real = text_real.split()\n",
    "    \n",
    "    from collections import Counter\n",
    "    Counter = Counter(text_fake)\n",
    "    count_fake = Counter.most_common()\n",
    "    \n",
    "    from collections import Counter\n",
    "    Counter = Counter(text_real)\n",
    "    count_real = Counter.most_common()\n",
    "    \n",
    "    df_real = pd.DataFrame(count_real, columns =['Name', 'freq_norm'])\n",
    "    df_fake = pd.DataFrame(count_fake, columns =['Name', 'freq_fake'])\n",
    "    \n",
    "    merged_df = pd.merge(df_fake, df_real, on=\"Name\", how='outer')\n",
    "    merged_df = merged_df.fillna(0)\n",
    "    merged_df['fake_rate'] = 0.0\n",
    "    \n",
    "    for i in range(len(merged_df)):\n",
    "        if merged_df['freq_fake'][i] > merged_df['freq_norm'][i]:\n",
    "            merged_df['fake_rate'][i] = ((merged_df['freq_fake'][i]+1) - (merged_df['freq_norm'][i]+1))/(merged_df['freq_norm'][i]+1)\n",
    "        else:\n",
    "            merged_df['fake_rate'][i] = -((merged_df['freq_norm'][i]+1) - (merged_df['freq_fake'][i]+1))/(merged_df['freq_fake'][i]+1)\n",
    "            \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "10e005e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_df = fake_rate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b4fe07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем словарь, в котором будут слова и их показатель фейковости\n",
    "word_dict = dict(zip(fr_df.Name, fr_df.fake_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f82c5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем вектроное представление каждого заголовка\n",
    "def vect(title):\n",
    "    vector = []\n",
    "    for key in word_dict:\n",
    "        if key in title:\n",
    "            vector.append(word_dict[key])\n",
    "        else:\n",
    "            vector.append(0.0)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5bd1995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].apply(lambda x: vect(x), 1)\n",
    "test_df['title'] = test_df['title'].apply(lambda x: vect(x), 1)\n",
    "\n",
    "X = df[['title']]\n",
    "test_df_x = test_df[['title']]\n",
    "\n",
    "y = df['is_fake']\n",
    "\n",
    "X = [X, pd.DataFrame(X['title'].tolist())]\n",
    "test_df_x = [test_df_x, pd.DataFrame(test_df_x['title'].tolist())]\n",
    "\n",
    "X = pd.concat(X, axis=1).drop('title', axis=1)\n",
    "test_df_x = pd.concat(test_df_x, axis=1).drop('title', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "15eb2346",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['sum'] = X.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "eae2e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_x['sum'] = test_df_x.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e7ac9a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[['sum']]\n",
    "test_df_x = test_df_x[['sum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "53026b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y.values.ravel(), test_size = 0.1, stratify=y, random_state=2)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "01a94d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.337830, F-Score=0.954, Precision=0.934, Recall=0.976\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, preds)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n",
    "                                                                        fscore[ix],\n",
    "                                                                        precision[ix],\n",
    "                                                                        recall[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d7397853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9593730927182902"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score = cross_val_score(model, X_train, Y_train, scoring='f1', cv=KFold(n_splits=10, shuffle=True, random_state=20))\n",
    "cv_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4896f814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>is_fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Роскомнадзор представил реестр сочетаний цвето...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ночью под Минском на президентской горе Белара...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Бывший спичрайтер Юрия Лозы рассказал о трудно...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Сельская церковь, собравшая рекордно низкое ко...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Акции Google рухнули после объявления о переза...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  is_fake\n",
       "0  Роскомнадзор представил реестр сочетаний цвето...        1\n",
       "1  Ночью под Минском на президентской горе Белара...        1\n",
       "2  Бывший спичрайтер Юрия Лозы рассказал о трудно...        1\n",
       "3  Сельская церковь, собравшая рекордно низкое ко...        1\n",
       "4  Акции Google рухнули после объявления о переза...        0"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = model.predict(test_df_x)\n",
    "predictions = pd.read_csv(TEST_DATASET_PATH, sep='\\t')\n",
    "predictions['is_fake'] = test_predictions\n",
    "predictions.to_csv(r'C:\\Users\\okruz\\Downloads\\Okruzhnov_Sergei\\pred.tsv', index=False)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b1a775b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    513\n",
       "1    487\n",
       "Name: is_fake, dtype: int64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.is_fake.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f68506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
